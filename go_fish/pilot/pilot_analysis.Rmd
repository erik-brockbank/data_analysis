---
title: "Go Fish Web Pilot"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyverse)
library(viridis)
```



```{r data_functions, include=FALSE}
SUMMARY_DATA = "01_go_fish_pilot_meta.csv"
TRIAL_DATA = "02_go_fish_pilot_trials.csv"
GENERATION_RESP_DATA = "03_go_fish_pilot_generation_free_resp.csv"
GENERATION_JUDG_DATA = "04_go_fish_pilot_generation_judgment.csv"
EVAL_DATA = "05_go_fish_pilot_evaluation.csv"
MEMORY_DATA = "06_go_fish_pilot_memory.csv"


read_data = function(filepath) {
  data = read_csv(filepath)
  # Process data here as needed
  return(data)
}

```



```{r summary_functions, include=FALSE}

get_prediction_summary = function(prediction_data) {
  prediction_summary = prediction_data %>%
    mutate(input_correct = 
             (input_prediction_catches_fish == prediction_catches_fish)) %>%
    group_by(is_control, trial_index) %>%
    summarize(mean_accuracy = sum(input_correct) / n())
  return(prediction_summary)
}


get_generation_judg_summary = function(generation_judg) {
  generation_judg_summary = generation_judg %>%
    mutate(input_correct = 
             (input_judgment == judgment_catches_fish)) %>%
    # NB: we only need subjID here but including it makes later group_by easier
    group_by(is_control, subjID) %>%
    summarize(subj_accuracy = sum(input_correct) / n()) %>%
    group_by(is_control) %>%
    summarize(mean_accuracy = mean(subj_accuracy),
              subjects = n(),
              se_accuracy = sd(subj_accuracy) / sqrt(n()),
              ci_lower = mean_accuracy - se_accuracy,
              ci_upper = mean_accuracy + se_accuracy)
  return(generation_judg_summary)
}


get_eval_summary = function(eval_data) {
  eval_summary_target_rule = eval_data %>%
    filter(is_target_rule == TRUE) %>% # for target rule, summarize across participants
    group_by(is_target_rule, is_control) %>%
    summarize(mean_rating = mean(input_rule_rating),
              subjects = n(),
              se_rating = sd(input_rule_rating) / sqrt(n()),
              ci_lower = mean_rating - se_rating,
              ci_upper = mean_rating + se_rating)
  
  eval_summary_other_rules = eval_data %>%
    filter(is_target_rule == FALSE) %>% # for target rule, summarize across participants
    group_by(is_target_rule, is_control, subjID) %>%
    summarize(mean_subj_rating = mean(input_rule_rating),
              rules = n()) %>%
    group_by(is_target_rule, is_control) %>%
    summarize(mean_rating = mean(mean_subj_rating),
              subjects = n(),
              se_rating = sd(mean_subj_rating) / sqrt(n()),
              ci_lower = mean_rating - se_rating,
              ci_upper = mean_rating + se_rating)
    
  eval_summary = rbind(eval_summary_target_rule, eval_summary_other_rules)
  return(eval_summary)
}


get_time_summary = function(time_data) {
  time_summary = time_data %>%
    mutate(experiment_completion_time = (expt_end_ts - expt_start_ts) / 1000) %>%
    group_by(is_control) %>%
    summarize(mean_task_time = mean(experiment_completion_time),
              subjects = n(),
              se_task_time = sd(experiment_completion_time) / sqrt(n()),
              ci_lower = mean_task_time - se_task_time,
              ci_upper = mean_task_time + se_task_time)
  return(time_summary)
}


get_memory_summary = function(memory_data) {
  memory_summary = memory_data %>%
    mutate(memory_correct = 
             (memory_shape_in_expt == input_shape_in_expt)) %>%
    # NB: we only need subjID here but including it makes later group_by easier
    group_by(is_control, subjID) %>%
    summarize(subj_accuracy = sum(memory_correct) / n()) %>%
    group_by(is_control) %>%
    summarize(mean_memory_accuracy = mean(subj_accuracy),
              subjects = n(),
              se_memory_accuracy = sd(subj_accuracy) / sqrt(n()),
              ci_lower = mean_memory_accuracy - se_memory_accuracy,
              ci_upper = mean_memory_accuracy + se_memory_accuracy)
  return(memory_summary)
}

```



```{r graphing_styles, include=FALSE}
CONDITION_LABELS = c("TRUE" = "describe", "FALSE" = "explain")
RULE_EVAL_LABELS = c("TRUE" = "target rule", "FALSE" = "all other rules")


individ_plot_theme = theme(
  # titles
  plot.title = element_text(face = "bold", size = 24),
  axis.title.y = element_text(face = "bold", size = 20),
  axis.title.x = element_text(face = "bold", size = 20),
  legend.title = element_text(face = "bold", size = 16),
  # axis text
  axis.text.y = element_text(size = 14),
  axis.text.x = element_text(size = 14),
  # legend text
  legend.text = element_text(size = 16),
  # facet text
  strip.text = element_text(size = 12),
  # backgrounds, lines
  panel.background = element_blank(),
  strip.background = element_blank(),
  
  panel.grid = element_line(color = "gray"),
  axis.line = element_line(color = "black"),
  # positioning
  legend.position = "bottom",
  legend.key = element_rect(colour = "transparent", fill = "transparent")
)

```



```{r graphing_functions, include=FALSE}
plot_prediction_summary = function(prediction_summary) {
  prediction_summary %>%
    ggplot(aes(x = trial_index, y = mean_accuracy, color = is_control)) +
    geom_line() +
    labs(x = "Trial index", y = "Mean accuracy") +
    # ggtitle("Prediction accuracy") +
    scale_color_viridis(discrete = T,
                        name = element_blank(),
                        labels = CONDITION_LABELS) +
    individ_plot_theme
}


plot_generation_judgments = function(generation_judg_summary) {
  generation_judg_summary %>%
    ggplot(aes(x = is_control, y = mean_accuracy, 
               color = is_control, fill = is_control)) +
    geom_bar(stat = "identity", alpha = 0.5, width = 0.5) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25, color = "black") +
    geom_hline(yintercept = 0.5, color = "red", size = 2, linetype = "dashed") +
    labs(x = "", y = "Mean accuracy") +
    # ggtitle("Accuracy on generation test") +
    scale_color_viridis(discrete = T,
                        name = element_blank(),
                        labels = CONDITION_LABELS) +
    scale_fill_viridis(discrete = T,
                        name = element_blank(),
                        labels = CONDITION_LABELS) +
    individ_plot_theme +
    theme(axis.text.x = element_blank())
}


plot_eval_results = function(eval_data) {
  eval_data %>%
    ggplot(aes(x = is_target_rule, y = mean_rating, 
               color = is_control, fill = is_control)) +
    geom_bar(stat = "identity", position = position_dodge(), width = 0.5, alpha = 0.5) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(), width = 0.5, color = "black") +
    labs(y = "Mean rating (1-7)") +
    # ggtitle("Evaluation across conditions") +
    scale_x_discrete(name = element_blank(),
                   labels = RULE_EVAL_LABELS) +
    scale_color_viridis(discrete = T,
                        name = element_blank(),
                        labels = CONDITION_LABELS) +
    scale_fill_viridis(discrete = T,
                       name = element_blank(),
                       labels = CONDITION_LABELS) +
    individ_plot_theme
}


plot_time_data = function(time_summary) {
  time_summary %>%
    ggplot(aes(x = is_control, y = mean_task_time, 
               color = is_control, fill = is_control)) +
    geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25, color = "black") +
    labs(x = "", y = "Mean time on expt (secs.)") +
    # ggtitle("Time on task across conditions") +
    scale_color_viridis(discrete = T,
                        name = element_blank(),
                        labels = CONDITION_LABELS) +
    scale_fill_viridis(discrete = T,
                       name = element_blank(),
                       labels = CONDITION_LABELS) +
    individ_plot_theme +
    theme(axis.text.x = element_blank())
}


plot_memory_data = function(memory_summary) {
  memory_summary %>%
    ggplot(aes(x = is_control, y = mean_memory_accuracy, 
               color = is_control, fill = is_control)) +
    geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25, color = "black") +
    labs(x = "", y = "Mean accuracy") +
    # ggtitle("Memory probe accuracy across conditions") +
    scale_color_viridis(discrete = T,
                        name = element_blank(),
                        labels = CONDITION_LABELS) +
    scale_fill_viridis(discrete = T,
                       name = element_blank(),
                       labels = CONDITION_LABELS) +
    individ_plot_theme +
    theme(axis.text.x = element_blank())
}


plot_generation_data = function(generation_data) {
  generation_data %>%
    group_by(is_control) %>%
    summarize(subjects = n(),
              correct_generation_pct = sum(free_response_coded) / n()) %>%
    ggplot(aes(x = is_control, y = correct_generation_pct, 
               color = is_control, fill = is_control)) +
      geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
      labs(x = "", y = "Percent correct rule generation") +
      # ggtitle("Rule generation across conditions") +
      scale_color_viridis(discrete = T,
                          name = element_blank(),
                          labels = CONDITION_LABELS) +
      scale_fill_viridis(discrete = T,
                         name = element_blank(),
                         labels = CONDITION_LABELS) +
      individ_plot_theme +
      theme(axis.text.x = element_blank())
}

```




# Intro
In this experiment, we look at the impact of explaining on hypothesis generation and evaluation processes. Across an *explain* and a *describe* condition, participants are presented with evidence about fishing lure combinations that do and don't catch fish. They respond to each piece of evidence (either explaining why it did/did not catch fish or describing it) and make a prediction about a similar fishing lure combination. 

Once they have seen all the evidence, participants are asked to describe the best rule they were able to come up with about which lure combinations catch fish (hypothesis generation). In a subsequent judgment task, they are presented with eight novel lure combinations and asked which ones will catch fish (test of hypothesis generation). 

Finally, participants are asked to evaluate six candidate rules for which lure combinations catch fish, including the target rule (which is consistent with all the evidence), as well as rules that vary in how well they describe the evidence. Upon completion of the task, participants perform a memory test for familiar and novel lure combinations. 

**We hypothesize that explaining will make participants more likely to generate the target hypothesis (which is consistent with the virtues of explanation) but that both conditions will evaluate the hypothesis similarly.**


# Results

We piloted with 17 participants across the two conditions.

```{r summary, echo=FALSE, message=FALSE}
summary_data = read_data(SUMMARY_DATA)
summary_data = summary_data %>%
  mutate(Condition = ifelse(is_control == TRUE, "Describe", "Explain"))
tab = table(summary_data$Condition)
kable(tab, col.names = c("Condition", "Participants"))
```


## Hypothesis Generation
First, we look at participant accuracy on the judgment task that followed their hypothesis generation. The plot below shows average accuracy across participants in each condition (error bars indicate one SEM).

```{r generation_judgment, echo=FALSE, message=FALSE}
# Analysis: generation judgment task accuracy by condition
generation_judg = read_data(GENERATION_JUDG_DATA)
generation_judg_summary = get_generation_judg_summary(generation_judg)
plot_generation_judgments(generation_judg_summary)
```


I've coded people's free response answers in the hypothesis generation task based on whether they accurately described the rule for which lure combinations catch fish.

```{r generation_free_response, echo=FALSE, message=FALSE}
generation_data = read_data(GENERATION_RESP_DATA)
generation_data = generation_data %>%
  mutate(free_response_coded = 0)

# Score generation data: only correct if it's very clear cut
generation_data$free_response_coded[1] = 1
generation_data$free_response_coded[2] = 1
generation_data$free_response_coded[3] = 0 # tough call here: had low accuracy on judgment task
generation_data$free_response_coded[4] = 0
generation_data$free_response_coded[5] = 1
generation_data$free_response_coded[6] = 1 # shape disjunction not pointyness
generation_data$free_response_coded[7] = 0 # tough call here: had fairly low accuracy on judgment task
generation_data$free_response_coded[8] = 0
generation_data$free_response_coded[9] = 0 # may have figured out rule but didn't supply it here
generation_data$free_response_coded[10] = 1 # interesting new response (more than one sideo on bottom)
generation_data$free_response_coded[11] = 1
generation_data$free_response_coded[12] = 0
generation_data$free_response_coded[13] = 1 # disjunction of larger round and smaller pointy or two pointy
generation_data$free_response_coded[14] = 0
generation_data$free_response_coded[15] = 0
generation_data$free_response_coded[16] = 0
generation_data$free_response_coded[17] = 0
```


Below is a plot of the percent of accurate responses across conditions (no error bars here because this is based on the raw count of correct hypotheses in each condition). 

This shows a decent number of people in the explain condition getting the rule (60-70%) and very few in the control condition getting it (10-15%). I suspect that with more people, we would see these numbers even out a bit since this seems like an extreme difference. 

```{r plot_generation_responses, echo=FALSE, message=FALSE}
plot_generation_data(generation_data)
```


To get a better sense of what these responses look like, below is the raw data along with my coding results (the `Correct` column indicates how I coded the answer).

```{r print_generation_responses, echo=FALSE, message=FALSE}
generation_data %>%
  mutate(Condition = ifelse(is_control == TRUE, "Describe", "Explain")) %>%
  rename("Subject" = subjID, "Response" = free_response, "Correct" = free_response_coded) %>%
  select(Subject, Condition, Response, Correct) %>%
  kable(.)

```


Finally, in line with Bonawitz & Griffiths (2010), we can also look at average performance on each prediction to see how participants are performing in their hypothesis generation trial by trial. This data is fairly noisy with so few participants but could be interesting with more subjects.

```{r predictions, echo=FALSE, message=FALSE}
# Analysis: prediction accuracy by condition, trial
prediction_data = read_data(TRIAL_DATA)
prediction_summary = get_prediction_summary(prediction_data)
plot_prediction_summary(prediction_summary)
```



**To summarize, in this pilot data it definitely looks like we have explainers doing a better job generating the abstract "pointy shape" hypothesis.**



## Hypothesis Evaluation

The next thing we look at is how participants in both conditions *evaluate* the target hypothesis, across conditions and relative to the other hypotheses. In the figure below, participant ratings were on a 1-7 scale, with 1 being *"Not good"* and 7 being *"Good"*.


```{r evaluation, echo=FALSE, message=FALSE}
# Analysis: evaluation ratings by condition
eval_data = read_data(EVAL_DATA)
eval_summary = get_eval_summary(eval_data)
plot_eval_results(eval_summary)
# Note all other rules here is average of subject average ratings (for X non-target rules), 
# whereas target rule is mean across subjects (since only one target rule)
```

Note also that the target rule statistics above are the mean of all participant ratings for the target rule, while the "all other rules" statistics are the mean and standard error of each subject's average rating for the five non-target rules (error bars are one standard error of the respective means). 


**Here, participants in both conditions evaluate the target explanation as similarly likely (though it appears the explainers may find it slightly better) and much better than competing explanations.**


## Time on Task
An important consideration is whether participants simply spent longer on the explanation task than the control description task and therefore performed better. 

Indeed, the figure below suggests that explainers spent longer on the experiment overall. For future versions, we'll isolate the time spent on the explanation/description trials rather than the whole experiment. However, it may be worth putting in place additional checks to limit time on task or exploit behaviors that the control describers may be better at. 


```{r completion_time, echo=FALSE, message=FALSE}
time_data = read_data(SUMMARY_DATA)
time_summary = get_time_summary(time_data)
plot_time_data(time_summary)
```



## Memory
Similar to time on task, it's worth looking at results from the memory task at the end of the experiment to address possible confounds from explainers having a better memory. 

In our experiment, we allow participants to view the relevant observations during the evaluation phase of the experiment. While they don't see them during the hypothesis generation phase, we don't expect that people are discovering the correct rule from memory during that phase.

In the graph below, we look at accuracy on the memory task. Participants were given a recognition task with four items from the experiment and four that were novel. The four from the experiment were selected to balance whether they appeared as evidence or prediction items and whether they caught fish or not. 

In the graph below, the two conditions are similar, suggesting no difference in memory for which stimuli were and were not shown during the experiment.

```{r memory_performance, echo=FALSE, message=FALSE}
memory_data = read_data(MEMORY_DATA)
memory_summary = get_memory_summary(memory_data)
plot_memory_data(memory_summary)
```






