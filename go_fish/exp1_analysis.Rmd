---
title: "Go Fish - Experiment 1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE)
library(knitr)
library(stringr)
library(tidyverse)
library(viridis)
```



```{r globals, include=FALSE}
# Data files
SUMMARY_DATA = "01_go_fish_meta.csv"
TRIAL_DATA = "02_go_fish_trials.csv"
GENERATION_RESP_DATA = "03_go_fish_generation_free_resp.csv"
GENERATION_JUDG_DATA = "04_go_fish_generation_judgment.csv"
EVAL_DATA = "05_go_fish_evaluation.csv"
MEMORY_DATA = "06_go_fish_memory.csv"

PILOT_SUMMARY_DATA = "pilot/01_go_fish_pilot_meta.csv"
PILOT_TRIAL_DATA = "pilot/02_go_fish_pilot_trials.csv"
PILOT_GENERATION_RESP_DATA = "pilot/03_go_fish_pilot_generation_free_resp.csv"
PILOT_GENERATION_JUDG_DATA = "pilot/04_go_fish_pilot_generation_judgment.csv"
PILOT_EVAL_DATA = "pilot/05_go_fish_pilot_evaluation.csv"
PILOT_MEMORY_DATA = "pilot/06_go_fish_pilot_memory.csv"

# Data labels
RULE_EVAL_LABELS = c("TRUE" = "Target rule", "FALSE" = "All other rules")
```



```{r analysis_functions, include=FALSE}
# Read in and process summary data file
read_summary_data = function(filepath, is_pilot) {
  # Read in data at filepath and add/modify columns as needed
  summary_data = read_csv(filepath)
  summary_data = summary_data %>%
    mutate(Condition = ifelse(is_control == TRUE, "Describe", "Explain"),
           Pilot = is_pilot,
           experiment_completion_time = (expt_end_ts - expt_start_ts) / 1000)
  return(summary_data)
}


# Read in and process data from explain/describe trials
read_trial_data = function(filepath, is_pilot) {
  trial_data = read_csv(filepath)
  trial_data = trial_data %>%
    mutate(
      Condition = ifelse(is_control == TRUE, "Describe", "Explain"),
      Pilot = is_pilot,
      input_correct =
             (input_prediction_catches_fish == prediction_catches_fish))
  return(trial_data)
}


# Read in and process data from generation "judgment" (binary response) task
read_generation_judgment_data = function(filepath, is_pilot) {
  generation_judgment_data = read_csv(filepath)
  generation_judgment_data = generation_judgment_data %>%
    mutate(
      Condition = ifelse(is_control == TRUE, "Describe", "Explain"),
      Pilot = is_pilot,
      input_correct = 
             (input_judgment == judgment_catches_fish))
  return(generation_judgment_data)
}


# Read in and process data from generation free response task
# NB: coding of free response answers is handled separately
read_generation_free_resp_data = function(filepath, is_pilot) {
  generation_free_resp_data = read_csv(filepath)
  generation_free_resp_data = generation_free_resp_data %>%
    mutate(
      Condition = ifelse(is_control == TRUE, "Describe", "Explain"),
      Pilot = is_pilot)
  
  generation_free_resp_data$free_response_str = str_replace_all(
    generation_free_resp_data$free_response, "\n" , "[newline]")
  
  return(generation_free_resp_data)
}


# Read in and process data from rule evaluation task
read_evaluation_data = function(filepath, is_pilot) {
  evaluation_data = read_csv(filepath)
  evaluation_data = evaluation_data %>%
    mutate(Condition = ifelse(is_control == TRUE, "Describe", "Explain"),
           Pilot = is_pilot)
  return(evaluation_data)
}


# Read in and process data from memory task at end of experiment
read_memory_data = function(filepath, is_pilot) {
  memory_data = read_csv(filepath)
  memory_data = memory_data %>%
    mutate(Condition = ifelse(is_control == TRUE, "Describe", "Explain"),
           Pilot = is_pilot,
           memory_correct = 
             (memory_shape_in_expt == input_shape_in_expt))
  return(memory_data)
}


# Summarize prediction data for graphing
get_prediction_summary = function(trial_data) {
  prediction_summary = trial_data %>%
    group_by(Condition, trial_index) %>%
    summarize(accuracy = sum(input_correct) / n())
  return(prediction_summary)
}


# Summarize generation judgment data by participant
get_generation_judgment_subj_summary = function(generation_judgment_data) {
  generation_subject_summary = generation_judgment_data %>%
    group_by(Condition, subjID) %>%
    summarize(subj_accuracy = sum(input_correct) / n())
  return(generation_subject_summary)
}


# Summarize generation judgment data across participants
get_generation_judgment_summary = function(generation_judgment_subject_summary) {
  generation_judg_summary = generation_judgment_subject_summary %>%
    group_by(Condition) %>%
    summarize(mean_accuracy = mean(subj_accuracy),
              subjects = n(),
              se_accuracy = sd(subj_accuracy) / sqrt(n()),
              ci_lower = mean_accuracy - se_accuracy,
              ci_upper = mean_accuracy + se_accuracy)
  return(generation_judg_summary)
}


# Summarize generation free response data across participants
get_generation_free_response_summary = function(generation_free_response_coded) {
  generation_free_response_summary = generation_free_response_coded %>%
    group_by(Condition) %>%
    summarize(subjects = n(),
              correct_generation_pct = sum(free_response_coded) / n())
  return(generation_free_response_summary)
}


# Summarize evaluation data across participants and conditions for target and non-target rules
get_evaluation_summary = function(evaluation_data) {
  # Average rating across participants on target rule
  eval_summary_target_rule = evaluation_data %>%
    filter(is_target_rule == TRUE) %>% # for target rule, summarize across participants
    group_by(is_target_rule, Condition) %>%
    summarize(mean_rating = mean(input_rule_rating),
              subjects = n(),
              se_rating = sd(input_rule_rating) / sqrt(n()),
              ci_lower = mean_rating - se_rating,
              ci_upper = mean_rating + se_rating)
  
  # Average of each participant's average across non-target rules
  eval_summary_other_rules = evaluation_data %>%
    filter(is_target_rule == FALSE) %>% # for target rule, summarize across participants
    group_by(is_target_rule, Condition, subjID) %>%
    summarize(mean_subj_rating = mean(input_rule_rating),
              rules = n()) %>%
    group_by(is_target_rule, Condition) %>%
    summarize(mean_rating = mean(mean_subj_rating),
              subjects = n(),
              se_rating = sd(mean_subj_rating) / sqrt(n()),
              ci_lower = mean_rating - se_rating,
              ci_upper = mean_rating + se_rating)
    
  eval_summary = rbind(eval_summary_target_rule, eval_summary_other_rules)
  return(eval_summary)
}


# Summarize experiment completion time data
get_time_summary = function(time_data) {
  time_summary = time_data %>%
    group_by(Condition) %>%
    summarize(mean_task_time = mean(experiment_completion_time),
              subjects = n(),
              se_task_time = sd(experiment_completion_time) / sqrt(subjects),
              ci_lower = mean_task_time - se_task_time,
              ci_upper = mean_task_time + se_task_time)
  return(time_summary)
}


# Summarize average trial completion time by participant
get_trial_time_subj_summary = function(trial_data) {
  trial_subject_summary = trial_data %>%
    mutate(trial_completion_time = (trial_n_end_ts - trial_n_start_ts) / 1000) %>%
    group_by(Pilot, Condition, subjID) %>%
    summarize(mean_trial_completion = mean(trial_completion_time),
              trials = n(),
              se_trial_completion = sd(trial_completion_time) / sqrt(trials),
              ci_lower = mean_trial_completion - se_trial_completion,
              ci_upper = mean_trial_completion + se_trial_completion)
  return(trial_subject_summary)
}


# Get summary of time spent on trials in each condition across participants
get_trial_time_summary = function(trial_time_subj_summary) {
  trial_time_summary = trial_time_subj_summary %>%
    # trial time completion not available in pilot data
    filter(Pilot == FALSE) %>%
    group_by(Condition) %>%
    # These column names kept the same as those in `get_time_summary` for easier graphing
    summarize(mean_task_time = mean(mean_trial_completion),
              subjects = n(),
              se_trial_time = sd(mean_trial_completion) / sqrt(subjects),
              ci_lower = mean_task_time - se_trial_time,
              ci_upper = mean_task_time + se_trial_time)
  return(trial_time_summary)
}


# Summarize memory performance data by participant
get_memory_subj_summary = function(memory_data) {
  memory_subj_accuracy = memory_data %>%
    group_by(Condition, subjID) %>%
    summarize(subj_accuracy = sum(memory_correct) / n())
  return(memory_subj_accuracy)
}


# Summarize memory performance across participants by condition
get_memory_summary = function(memory_subject_summary) {
  memory_summary = memory_subject_summary %>%
    group_by(Condition) %>%
    summarize(mean_memory_accuracy = mean(subj_accuracy),
              subjects = n(),
              se_memory_accuracy = sd(subj_accuracy) / sqrt(n()),
              ci_lower = mean_memory_accuracy - se_memory_accuracy,
              ci_upper = mean_memory_accuracy + se_memory_accuracy)
  return(memory_summary)
}


# Hand-code participant responses on free response rule generation data
code_generation_free_resp_data = function(generation_free_resp_data) {
  # Add column for coding of free response (1 = definitely accurate rule, 0 otherwise)
  generation_free_resp_coded = generation_free_resp_data %>%
    mutate(free_response_coded = 0)

  # Code free response data
  # Comments:
    # Unsure on 9, 14 (similar to 9), 27, 29, 35 (similar to 9, 14)
  generation_free_resp_coded$free_response_coded[2] = 1
  generation_free_resp_coded$free_response_coded[6] = 1
  generation_free_resp_coded$free_response_coded[7] = 1
  generation_free_resp_coded$free_response_coded[8] = 1
  generation_free_resp_coded$free_response_coded[10] = 1
  generation_free_resp_coded$free_response_coded[11] = 1
  generation_free_resp_coded$free_response_coded[19] = 1
  generation_free_resp_coded$free_response_coded[20] = 1
  generation_free_resp_coded$free_response_coded[23] = 1
  generation_free_resp_coded$free_response_coded[24] = 1
  generation_free_resp_coded$free_response_coded[25] = 1
  generation_free_resp_coded$free_response_coded[26] = 1
  generation_free_resp_coded$free_response_coded[31] = 1
  generation_free_resp_coded$free_response_coded[34] = 1
  generation_free_resp_coded$free_response_coded[36] = 1
  generation_free_resp_coded$free_response_coded[37] = 1
  generation_free_resp_coded$free_response_coded[40] = 1
  generation_free_resp_coded$free_response_coded[43] = 1
  generation_free_resp_coded$free_response_coded[44] = 1
  generation_free_resp_coded$free_response_coded[45] = 1
  generation_free_resp_coded$free_response_coded[47] = 1
  generation_free_resp_coded$free_response_coded[48] = 1
  generation_free_resp_coded$free_response_coded[55] = 1
  generation_free_resp_coded$free_response_coded[58] = 1
  generation_free_resp_coded$free_response_coded[63] = 1
  # Pilot coding
  generation_free_resp_coded$free_response_coded[70] = 1
  generation_free_resp_coded$free_response_coded[71] = 1
  generation_free_resp_coded$free_response_coded[74] = 1
  generation_free_resp_coded$free_response_coded[75] = 1
  generation_free_resp_coded$free_response_coded[79] = 1
  generation_free_resp_coded$free_response_coded[80] = 1
  generation_free_resp_coded$free_response_coded[82] = 1
  
  return(generation_free_resp_coded)
}

```



```{r graphing_functions, include=FALSE}
# Generic plot theme for use in most graphs
individ_plot_theme = theme(
  # titles
  plot.title = element_text(face = "bold", size = 24),
  axis.title.y = element_text(face = "bold", size = 20),
  axis.title.x = element_text(face = "bold", size = 20),
  legend.title = element_text(face = "bold", size = 16),
  # axis text
  axis.text.y = element_text(size = 14),
  axis.text.x = element_text(size = 14),
  # legend text
  legend.text = element_text(size = 16),
  # facet text
  strip.text = element_text(size = 12),
  # backgrounds, lines
  panel.background = element_blank(),
  strip.background = element_blank(),
  panel.grid = element_line(color = "gray"),
  axis.line = element_line(color = "black"),
  # positioning
  legend.position = "bottom",
  legend.key = element_rect(colour = "transparent", fill = "transparent")
)


# Line plot of prediction accuracy by condition for each trial
# (percent of participants in each condition who were correct in each trial)
plot_prediction_summary = function(prediction_summary) {
  prediction_summary %>%
    # filter(trial_index > 4) %>%
    ggplot(aes(x = trial_index, y = accuracy, color = Condition)) +
    geom_line() +
    geom_hline(yintercept = 0.5, linetype = "dashed") +
    labs(x = "Trial index", y = "Accuracy") +
    # ggtitle("Prediction accuracy in each round") +
    scale_color_viridis(discrete = T,
                        name = element_blank()) +
    individ_plot_theme
}


# Bar chart of classification accuracy on binary generation judgment task by condition
plot_generation_judgments = function(generation_judgment_summary) {
  generation_judgment_summary %>%
    ggplot(aes(x = Condition, y = mean_accuracy, 
               color = Condition, fill = Condition)) +
    geom_bar(stat = "identity", alpha = 0.5, width = 0.5) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
    geom_hline(yintercept = 0.5, linetype = "dashed") +
    labs(x = "", y = "Mean accuracy") +
    # ggtitle("Accuracy on generation judgment task") +
    scale_color_viridis(discrete = T,
                        name = element_blank()) +
    scale_fill_viridis(discrete = T,
                        name = element_blank()) +
    individ_plot_theme +
    theme(axis.text.x = element_blank())
}


# Bar chart of rule generation accuracy for coded free response answers
plot_generation_free_responses = function(generation_free_resp_summary) {
  generation_free_resp_summary %>%
    ggplot(aes(x = Condition, y = correct_generation_pct, 
               color = Condition, fill = Condition)) +
      geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
      labs(x = "", y = "Percent Correct") +
      # ggtitle("Rule generation across conditions") +
      scale_color_viridis(discrete = T,
                          name = element_blank()) +
      scale_fill_viridis(discrete = T,
                         name = element_blank()) +
      individ_plot_theme +
      theme(axis.text.x = element_blank())
}


# Bar chart of average evaluation ratings across conditions on rule evaluation task
plot_evaluation_results = function(evaluation_summary) {
  evaluation_summary %>%
    ggplot(aes(x = is_target_rule, y = mean_rating, 
               color = Condition, fill = Condition)) +
    geom_bar(stat = "identity", position = position_dodge(preserve = "single"), width = 0.5, alpha = 0.5) +
    geom_errorbar(
      aes(ymin = ci_lower, ymax = ci_upper), 
      position = position_dodge(width = 0.5, preserve = "single"), 
      width = 0.2) +
    labs(y = "Mean rating (1-7)") +
    # ggtitle("Evaluation across conditions") +
    scale_x_discrete(name = element_blank(),
                   labels = RULE_EVAL_LABELS) +
    scale_color_viridis(discrete = T,
                        name = element_blank()) +
    scale_fill_viridis(discrete = T,
                       name = element_blank()) +
    individ_plot_theme
}


# Bar chart of experiment completion time
plot_time_data = function(time_summary, ylab) {
  time_summary %>%
    ggplot(aes(x = Condition, y = mean_task_time, 
               color = Condition, fill = Condition)) +
    geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
    labs(x = "", y = ylab) +
    # ggtitle("Time on task across conditions") +
    scale_color_viridis(discrete = T,
                        name = element_blank()) +
    scale_fill_viridis(discrete = T,
                       name = element_blank()) +
    individ_plot_theme +
    theme(axis.text.x = element_blank())
}


# Bar chart of average memory accuracy across conditions 
plot_memory_data = function(memory_summary) {
  memory_summary %>%
    ggplot(aes(x = Condition, y = mean_memory_accuracy, 
               color = Condition, fill = Condition)) +
    geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
    geom_hline(yintercept = 0.5, linetype = "dashed") +
    labs(x = "", y = "Mean accuracy") +
    # ggtitle("Memory probe accuracy across conditions") +
    scale_color_viridis(discrete = T,
                        name = element_blank()) +
    scale_fill_viridis(discrete = T,
                       name = element_blank()) +
    individ_plot_theme +
    theme(axis.text.x = element_blank())
}
```



```{r initialize_data, include=FALSE}
# Read in data
summary_data = bind_rows(read_summary_data(SUMMARY_DATA, FALSE), 
                         read_summary_data(PILOT_SUMMARY_DATA, TRUE))
trial_data = bind_rows(read_trial_data(TRIAL_DATA, FALSE), 
                        read_trial_data(PILOT_TRIAL_DATA, TRUE))
generation_judgment_data = bind_rows(read_generation_judgment_data(GENERATION_JUDG_DATA, FALSE),
                                 read_generation_judgment_data(PILOT_GENERATION_JUDG_DATA, TRUE))
evaluation_data = bind_rows(read_evaluation_data(EVAL_DATA, FALSE),
                        read_evaluation_data(PILOT_EVAL_DATA, TRUE))
memory_data = bind_rows(read_memory_data(MEMORY_DATA, FALSE),
                        read_memory_data(MEMORY_DATA, TRUE))

# Summarize data
prediction_summary = get_prediction_summary(trial_data)
generation_judgment_subject_summary = get_generation_judgment_subj_summary(generation_judgment_data)
generation_judgment_summary = get_generation_judgment_summary(generation_judgment_subject_summary)
evaluation_summary = get_evaluation_summary(evaluation_data)

completion_time_summary = get_time_summary(summary_data)
trial_time_subject_summary = get_trial_time_subj_summary(trial_data)
trial_time_summary = get_trial_time_summary(trial_time_subject_summary)
memory_subject_summary = get_memory_subj_summary(memory_data)
memory_summary = get_memory_summary(memory_subject_summary)


# Read in and code free response data
generation_free_resp_data = bind_rows(read_generation_free_resp_data(GENERATION_RESP_DATA, FALSE),
                                  read_generation_free_resp_data(PILOT_GENERATION_RESP_DATA, TRUE))
generation_free_resp_coded = code_generation_free_resp_data(generation_free_resp_data)
generation_free_resp_summary = get_generation_free_response_summary(generation_free_resp_coded)
```



# Methods #

### Participants ###

We ran a total of 86 participants across the two conditions.


```{r participant_overview, echo=FALSE}
tab = table(summary_data$Condition)
kable(tab, col.names = c("Condition", "Participants"))
```


# Results #


### Hypothesis Generation ###

As a first indication of hypothesis generation across the two conditions, we look at performance on the classification task participants were given after the trials were complete. Participants were shown eight novel lures and asked to indicate whether each lure would catch fish, along with their confidence. The plot below shows average accuracy across participants in each condition, with error bars indicating one SEM.


```{r generation_judgment_analysis, echo=FALSE}
# t-test
t_gen = t.test(
  generation_judgment_subject_summary$subj_accuracy[generation_judgment_subject_summary$Condition == "Describe"],
  generation_judgment_subject_summary$subj_accuracy[generation_judgment_subject_summary$Condition == "Explain"]
)

# Plot results
plot_generation_judgments(generation_judgment_summary)
```


The difference between the two conditions was significant (*t* = `r round(t_gen$statistic, 2)`, *p* = `r round(t_gen$p.value, 3)`), suggesting that participants in the Explain condition were better able to generate the correct hypothesis.


Prior to the classification task above, participants provided free response answers about the best rule they used to determine which ures catch fish. These free response asnwers were coded for accuracy (the original answers and their coded accuracy are in the Appendix). The figure below shows accuracy of participant free response rule generation by condition.


```{r generation_free_resp_analysis, echo=FALSE}
# chi-sq
generation_props = generation_free_resp_coded %>%
  group_by(Condition) %>%
  summarize(success = sum(free_response_coded),
            total = n())

chisq_gen = prop.test(c(generation_props$success), c(generation_props$total))  

# Plot results
plot_generation_free_responses(generation_free_resp_summary)
```


The proportion of participants generating the correct rule was significantly higher in the Explain condition ( $\chi^2$ (`r chisq_gen$parameter`) = `r round(chisq_gen$statistic, 2)`, *p* = `r round(chisq_gen$p.value, 3)`), suggesting that control participants in the earlier classificatino task may in fact benefit from rough intuitions about which lures catch fish but that when asked to explicitly state the rule the difference between the conditions widens.


In a more-fine grained attempt to detect whether participants in the Explain condition may have figured out the rule sooner, we examine overall accuracy (percent of correct participants) in each condition on the prediction trials following each piece of evidence.

Here we see some evidence of explainers figuring out the rule sooner than control participants but both groups perform similarly on the final trial so this indication is fairly noisy.


```{r prediction_analysis, echo=FALSE}
# Plot results
plot_prediction_summary(prediction_summary)

# Fit regressions to data just to ensure no signal
mod_exp = lm(data = prediction_summary[prediction_summary$Condition == "Explain",], accuracy ~ trial_index)
mod_des = lm(data = prediction_summary[prediction_summary$Condition == "Describe",], accuracy ~ trial_index)

```


Indeed a linear regression fit to each condition's accuracy data over the eight trials does not produce slopes reliably different from 0 (Explain: *t* = `r round(summary(mod_exp)[["coefficients"]][, "t value"][2], 2)`, *p* = `r round(summary(mod_exp)[["coefficients"]][, "Pr(>|t|)"][2], 3)`; Describe: *t* = `r round(summary(mod_des)[["coefficients"]][, "t value"][2], 2)`, *p* = `r round(summary(mod_des)[["coefficients"]][, "Pr(>|t|)"][2], 3)`).


### Hypothesis Evaluation ###

To compare hypothesis evaluation results across conditions, we look at average evaluation of the target rule across participants in each condition and average of each participant's average rating for all non-target rules (error bars are one standard error of the respective means).

In the figure below, participant ratings were on a 1-7 scale, with 1 being *"Not good"* and 7 being *"Good"*. Across both conditions, participants rated the target rule highly and the others far less so. 

[TODO examine correlation between evidence and rating].


```{r evaluation_analysis, echo=FALSE}
# t-test
t_eval = t.test(
  evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe" & evaluation_data$is_target_rule == TRUE],
  evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain" & evaluation_data$is_target_rule == TRUE]
)

# Plot results
plot_evaluation_results(evaluation_summary)
```


The difference between the two evaluation ratings for the target rule was significant, with explain participants evaluating the target rule as a better rule (*t* = `r round(t_eval$statistic, 2)`, *p* = `r round(t_eval$p.value, 3)`). Because ratings are close to ceiling across both conditions, it's clear that both groups evaluate the rule highly despite the difference in each group's ability to generate the correct rule.


### Memory Performance ###

One might argue that rule generation in the Explain condition reflects deeper processing of the trial data, which may have allowed participants to perform better when generating hypotheses. If this is the case, we might expect participants in the Explain condition to remember lures seen during the experiment (and detect novel ones) better than control participants. 

The figure below shows performance on a memory task at the end of the experiment across conditions. Participants were asked to identify whether they had previously seen each lure in a set of eight, four of which were selected from throughout the experiment and four of which were novel.


```{r memory_analysis, echo=FALSE}
# t-test
t_mem = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Describe"],
               memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Explain"])

# Plot results
plot_memory_data(memory_summary)
```


Overall accuracy on the memory task does not differ significantly between conditions (*t* = `r round(t_mem$statistic, 2)`, *p* = `r round(t_mem$p.value, 3)`).


### Time on Task ###

Another important consideration with regard to participants' ability to generate the correct rule is whether participants in the Explain condition simply spent longer on the task and were therefore more likely to generate the right hypothesis. 

The figure below shows average time spent on the full experiment for participants across each condition.


```{r time_completion_analysis, echo=FALSE}
# t-test
t_time = t.test(summary_data$experiment_completion_time[summary_data$Condition == "Describe"],
                summary_data$experiment_completion_time[summary_data$Condition == "Explain"])

# Plot results
plot_time_data(completion_time_summary, ylab = "Mean time on experiment (sec.)")
```


Though it appears participants in the control condition may have spent *more* time on the experiment, this difference is not significant (*t* = `r round(t_time$statistic, 2)`, *p* = `r round(t_time$p.value, 3)`).

Diving further into the time spent on the *evidence trials within the experiment* (the only segment which differentiates the two conditions), the figure below shows average time spent on the eight evidence trials across participants in each condition (error bars indicate one standard error of participant means).


```{r trial_time_completion_analysis, echo=FALSE}
t_trials = t.test(trial_time_subject_summary$mean_trial_completion[trial_time_subject_summary$Condition == "Describe" &
                                                                     trial_time_subject_summary$Pilot == FALSE],
                  trial_time_subject_summary$mean_trial_completion[trial_time_subject_summary$Condition == "Explain" &
                                                                     trial_time_subject_summary$Pilot == FALSE])
# Plot results
plot_time_data(trial_time_summary, ylab = "Mean time on trials (sec.)")

```


Once again the difference between conditions is not in fact significant and in any case trends in the opposite of the direction expected if participants in the explain condition were generating the correct hypothesis more in light of having spent more time on the task (*t* = `r round(t_trials$statistic, 2)`, *p* = `r round(t_trials$p.value, 3)`).


# Appendix #

### Coding Hypothesis Generation Free Response ###


Below are the coded responses for whether participants gave correct rules when prompted to supply the best rule they used for determining which lures catch fish. A $1$ in the *Correct* column indicates that they were scored as providing a correct answer.


```{r generation_coding_results, echo=FALSE}
generation_free_resp_coded %>%
  select(subjID, Pilot, free_response_str, free_response_coded) %>%
  kable(., col.names = c("Subject", "Pilot", "Response", "Correct"), row.names = TRUE)
```


